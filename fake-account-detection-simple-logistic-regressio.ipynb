{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-02-07T10:24:50.280067Z","iopub.execute_input":"2023-02-07T10:24:50.280528Z","iopub.status.idle":"2023-02-07T10:24:50.293739Z","shell.execute_reply.started":"2023-02-07T10:24:50.280477Z","shell.execute_reply":"2023-02-07T10:24:50.292289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Scenario\n#### A popular social media platform for sharing photos and videos, has received complaints about fake user accounts. These fake accounts are said to have left spam comments on genuine user posts. Management has asked us to create a machine learning model that will help the platform distinguish real accounts from fake accounts. The company would then use the model to identify fake accounts so they can subsequently be deleted from the platform.\n![FAKE SOCIAL MEDIA ACCOUNTS and ITâ€™S CONCERN?](https://www.endnowfoundation.org/wp-content/uploads/elementor/thumbs/Detect-Fake-Profiles-on-Social-Media-p6yfct3ismgslao8tyklprwyrfd5tttfiwrd6xcjuw.jpg)\n##### Image taken from: https://www.endnowfoundation.org/detect-fake-profiles-on-social-media-php/\n##### Data about real and fake user accounts in social_media_train.csv.","metadata":{}},{"cell_type":"markdown","source":"# 1) Gather Data\n#### The data is in the ocial_media_train.csv file. The target vector is given by the 'fake' column. Here the modules that typically is needed for reading and exploration is imported and then is read in pandas DataFrame df_train.","metadata":{}},{"cell_type":"code","source":"# Import modules \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2023-02-07T10:50:37.117411Z","iopub.execute_input":"2023-02-07T10:50:37.118096Z","iopub.status.idle":"2023-02-07T10:50:37.126460Z","shell.execute_reply.started":"2023-02-07T10:50:37.118045Z","shell.execute_reply":"2023-02-07T10:50:37.125406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read data\ndf = pd.read_csv(\"/kaggle/input/social-media-train/social_media_train.csv\", index_col=[0])\ndf.info()","metadata":{"execution":{"iopub.status.busy":"2023-02-07T10:51:51.297184Z","iopub.execute_input":"2023-02-07T10:51:51.297606Z","iopub.status.idle":"2023-02-07T10:51:51.325908Z","shell.execute_reply.started":"2023-02-07T10:51:51.297571Z","shell.execute_reply":"2023-02-07T10:51:51.324891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2) Dataset Description \n","metadata":{}},{"cell_type":"code","source":"data_dict = pd.read_csv('/kaggle/input/fake-account-data-dict/fake_account__data_dict.csv', index_col = 'No.')\ndata_dict","metadata":{"execution":{"iopub.status.busy":"2023-02-07T10:57:46.849229Z","iopub.execute_input":"2023-02-07T10:57:46.849780Z","iopub.status.idle":"2023-02-07T10:57:46.868734Z","shell.execute_reply.started":"2023-02-07T10:57:46.849739Z","shell.execute_reply":"2023-02-07T10:57:46.867159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3) Goal of this project\n##### The goal here is to predict whether a user account is fake or not. A problem of this nature is called a binary classification problem (binary since we have two categories). We use int numbers to specify the two categories. In the 'fake' column, a 1 represents that the account in that row is fake, while a 0 indicates a real account. ","metadata":{}},{"cell_type":"markdown","source":"# 4) Exploratory Data Analysis (EDA)\n## Understand Data\n#### It is necessary to familiarize with the data at the beginning so that we know later what to look for while cleaning and preparing the data.","metadata":{}},{"cell_type":"code","source":"# Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Suppress scientific notation\nnp.set_printoptions(suppress=True) \npd.options.display.float_format = '{:.2f}'.format\n\n# Display all columns\npd.set_option('display.max_columns', None)\n\n# Check first five rows of data\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-02-07T10:54:12.496487Z","iopub.execute_input":"2023-02-07T10:54:12.496976Z","iopub.status.idle":"2023-02-07T10:54:12.516571Z","shell.execute_reply.started":"2023-02-07T10:54:12.496936Z","shell.execute_reply":"2023-02-07T10:54:12.515254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Each line of df represents a user or user account.","metadata":{}},{"cell_type":"code","source":"#check describe\ndf.describe()","metadata":{"execution":{"iopub.status.busy":"2023-02-07T11:08:26.451052Z","iopub.execute_input":"2023-02-07T11:08:26.451709Z","iopub.status.idle":"2023-02-07T11:08:26.497131Z","shell.execute_reply.started":"2023-02-07T11:08:26.451657Z","shell.execute_reply":"2023-02-07T11:08:26.495944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Determine categorical and numerical features\n\n# Numerical columns\nnum_cols = ['ratio_numlen_username', 'len_fullname', 'ratio_numlen_fullname',\n                'len_desc', 'num_posts', 'num_followers', 'num_following']\n# Categorical columns\ncat_cols = [col for col in df.columns.values.tolist() if col not in num_cols]\ncat_cols","metadata":{"execution":{"iopub.status.busy":"2023-02-07T11:21:18.065967Z","iopub.execute_input":"2023-02-07T11:21:18.066396Z","iopub.status.idle":"2023-02-07T11:21:18.075851Z","shell.execute_reply.started":"2023-02-07T11:21:18.066364Z","shell.execute_reply":"2023-02-07T11:21:18.074506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get an idea of target category: fake\nplt.figure(figsize=(15,6))\nfake_share = df[\"fake\"].value_counts()\nmylabel=[\"Not fake(0)\",\"fake(1)\"]\ncolors = ['#99ff99','#ff9999']\nplt.pie(fake_share,\n        labels=mylabel,autopct=\"%1.1f%%\",colors=colors,\n        textprops={'fontsize': 16})\nplt.axis(\"equal\");","metadata":{"execution":{"iopub.status.busy":"2023-02-07T11:23:53.332517Z","iopub.execute_input":"2023-02-07T11:23:53.333028Z","iopub.status.idle":"2023-02-07T11:23:53.517347Z","shell.execute_reply.started":"2023-02-07T11:23:53.332989Z","shell.execute_reply":"2023-02-07T11:23:53.515898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Target category is strongly balanced. ","metadata":{}},{"cell_type":"code","source":"# Check the percentage of the missing values\n\npercent_missing = df.isnull().sum() * 100 / len(df)\nmissing_value_df = pd.DataFrame({'percent_missing (%)': percent_missing})\nmissing_value_df.sort_values('percent_missing (%)', ascending=False)","metadata":{"execution":{"iopub.status.busy":"2023-02-07T11:25:59.531030Z","iopub.execute_input":"2023-02-07T11:25:59.531767Z","iopub.status.idle":"2023-02-07T11:25:59.548565Z","shell.execute_reply.started":"2023-02-07T11:25:59.531714Z","shell.execute_reply":"2023-02-07T11:25:59.547231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check data correlation: Sort most highly correlated values\ndisplay(df.corr()['fake'].sort_values())\n\n# Correlation heatmap\n# Colormap: Most negative correlations (dark-blue) to most positive correlation (dark red)\ncorr = df.corr()\ncorr.style.background_gradient(cmap='coolwarm')","metadata":{"execution":{"iopub.status.busy":"2023-02-07T11:30:34.089273Z","iopub.execute_input":"2023-02-07T11:30:34.090524Z","iopub.status.idle":"2023-02-07T11:30:34.188932Z","shell.execute_reply.started":"2023-02-07T11:30:34.090474Z","shell.execute_reply":"2023-02-07T11:30:34.187625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" #### Just like linear regression, logistic regression makes a number of assumptions. For continuous data, the following are relevant:\n#### *  The features should not be strongly correlated with each other. \n#### *  There should be a linear relationship between the features and the sigmoid transformed probabilities.\n\n#### As can be seen all correlation values between features relatively close to 0, rark blue (First criteria above fulfilled). Number of characters in the account description (len_desc)and Ratio of numeric characters in the account useername (ratio_numlen_username) shows most positive and negative correlation with fake status.","metadata":{}},{"cell_type":"code","source":"# Categorical data\ndisplay(df.loc[:, cat_cols].head(10))\nprint('----------------------')\n\n# Unique values\nfor col in cat_cols:\n    unique_values = df.loc[:, col].unique()\n    print(\"\\nColumn name: {}\\nUnique values: {}\".format(col, unique_values))  ","metadata":{"execution":{"iopub.status.busy":"2023-02-07T12:01:15.285747Z","iopub.execute_input":"2023-02-07T12:01:15.286218Z","iopub.status.idle":"2023-02-07T12:01:15.305608Z","shell.execute_reply.started":"2023-02-07T12:01:15.286177Z","shell.execute_reply":"2023-02-07T12:01:15.304235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5) One-hot encoding\n#### Most machine learning models can only deal with numeric features. As like in this case for categorical columns above, many important real-world features are not numeric but rather categorical. \n#### The categorical features need to be transformed into numerical features. While numerous techniques exist to transform these features, the most common technique is one-hot encoding. 1-of-n encoding uses pdp.OneHotEncode() to create a set of new 0/1 features from a categorical feature with more than two categories","metadata":{}},{"cell_type":"code","source":"import pdpipe as pdp\n\n# Label encoding for categorical with two unique values\ndict_label_encoding = {'Yes': 1, 'No': 0}\ndf.loc[:, 'profile_pic'] = df.loc[:, 'profile_pic'].replace(dict_label_encoding)\ndf.loc[:, 'extern_url'] = df.loc[:, 'extern_url'].replace(dict_label_encoding)\ndf.loc[:, 'private'] = df.loc[:, 'private'].replace(dict_label_encoding)\n\n\n# one-hot encoding \nonehot = pdp.OneHotEncode([\"sim_name_username\"], drop_first=False) \n\n#fit and transform on train set\ndf = onehot.fit_transform(df) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Overview of train set\ndf","metadata":{"execution":{"iopub.status.busy":"2023-02-07T12:19:14.013400Z","iopub.execute_input":"2023-02-07T12:19:14.013921Z","iopub.status.idle":"2023-02-07T12:19:14.039243Z","shell.execute_reply.started":"2023-02-07T12:19:14.013881Z","shell.execute_reply":"2023-02-07T12:19:14.037971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6) Logistic regression without regularization\n#### we can now set up a logistic regression model, see Linear regression versus logistic regression. By default, sklearn's logistic regression algorithm already uses regularization <a href=\"https://www.simplilearn.com/tutorials/machine-learning-tutorial/regularization-in-machine-learning#:~:text=REGISTER%20NOW-,What%20is%20Regularization%20in%20Machine%20Learning%3F,-Regularization%20refers%20to\">regularization</a>, by default with the regularization parameter C=1.0. If we assign an extremely large value to C, such as a 1 followed by 42 zeros (1e42), no regularization is performed. That's what we want to achieve here first.\n\n#### Unfortunately, the algorithm needs many attempts to solve the problem. The default 100 iterations are not enough. Therefore, we should also assign a relatively large number to max_iter. This parameter sets the maximum number of iterations the solvers need to converge. 10000 (1e4) should suffice here.\n","metadata":{}},{"cell_type":"code","source":"# Import logistic regression library\nfrom sklearn.linear_model import LogisticRegression\n\n# Initiate model\nmodel_logreg = LogisticRegression(solver='lbfgs', max_iter=1e4, C=1e42, random_state=42)\n\n# Define feature and target values\nfeature_train = df.drop(df['fake'])\ntarget_train = df['fake']\n\n","metadata":{"execution":{"iopub.status.busy":"2023-02-07T12:35:55.968995Z","iopub.execute_input":"2023-02-07T12:35:55.969502Z","iopub.status.idle":"2023-02-07T12:35:55.979923Z","shell.execute_reply.started":"2023-02-07T12:35:55.969460Z","shell.execute_reply":"2023-02-07T12:35:55.978404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}