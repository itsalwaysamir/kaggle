{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input/loans-data/loans_data.csv'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n# Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Display all columns\npd.set_option('display.max_columns', None)\n\n# Ignore the resulting DataConversionWarning \nimport warnings\nfrom sklearn.exceptions import DataConversionWarning\nwarnings.filterwarnings(action='ignore', category=DataConversionWarning)\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-02-27T12:51:59.587005Z","iopub.execute_input":"2023-02-27T12:51:59.589725Z","iopub.status.idle":"2023-02-27T12:51:59.630548Z","shell.execute_reply.started":"2023-02-27T12:51:59.589642Z","shell.execute_reply":"2023-02-27T12:51:59.628566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import platform; print(platform.platform())\nimport sys; print(\"Python\", sys.version)\nimport numpy; print(\"NumPy\", numpy.__version__)\nimport scipy; print(\"SciPy\", scipy.__version__)\nimport sklearn; print(\"Scikit-Learn\", sklearn.__version__)","metadata":{"execution":{"iopub.status.busy":"2023-02-27T12:38:58.438756Z","iopub.execute_input":"2023-02-27T12:38:58.439706Z","iopub.status.idle":"2023-02-27T12:38:58.943097Z","shell.execute_reply.started":"2023-02-27T12:38:58.439641Z","shell.execute_reply":"2023-02-27T12:38:58.941371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Scenario\nWe work for LendingClub which is a US peer-to-peer lending company, headquartered in San Francisco, California. This company loans to private individuals. Investors indicate how much money they want to make available and the system pools money from different investors and forwards this money to people who are looking for a loan. The people who take out a loan often have a low credit rating, which is why they do not get the loan in the classic way from a bank. Particularly risky loans receive an interest rate of over 14%. Such loans are classified internally as problem loans and require close attention from LendingClub.\n\nPreviously, a service provider calculated the interest rate for each loan. The services of the external provider are now to be taken over step by step by internal departments. This saves costs and makes the assessment more transparent for LendingClub.\n\n## Goal\nThe task is to automatically classify the loans into problem loans and normal loans.\n\nLoan data has been acquired from LendingClub official website and US state codes and their names acquired from Wikipedia.\n\n![](https://m.bankingexchange.com/media/k2/items/cache/c852cd1706018952eef5203293178d22_XL.jpg?t=20170208_004958)\n\nImage source: https://m.bankingexchange.com\n\n","metadata":{}},{"cell_type":"markdown","source":"# Gather Data","metadata":{}},{"cell_type":"code","source":"#Import regured modules \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# suppress scientific notation\nnp.set_printoptions(suppress=True) \npd.options.display.float_format = '{:.6f}'.format\n\n# Read Data\ndf = pd.read_csv('/kaggle/input/loans-data/loans_data.csv')\ndf = df.drop('Unnamed: 0',axis=1)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-02-27T12:38:58.946573Z","iopub.execute_input":"2023-02-27T12:38:58.948187Z","iopub.status.idle":"2023-02-27T12:39:02.653775Z","shell.execute_reply.started":"2023-02-27T12:38:58.948117Z","shell.execute_reply":"2023-02-27T12:39:02.652500Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Shape of the data \nprint('Number of rows: ', df.shape[0])\nprint('Number of columns: ', df.shape[1])","metadata":{"execution":{"iopub.status.busy":"2023-02-27T12:39:02.655063Z","iopub.execute_input":"2023-02-27T12:39:02.655513Z","iopub.status.idle":"2023-02-27T12:39:02.663012Z","shell.execute_reply.started":"2023-02-27T12:39:02.655472Z","shell.execute_reply":"2023-02-27T12:39:02.661621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Dictionary","metadata":{}},{"cell_type":"code","source":"# Import and read data dictionary to get familiar with data coumn names and their description.\nimport pandas as pd \ndata_dict = pd.read_csv('/kaggle/input/d/iamamir/loans-data-dict/loans_data_dict.csv',index_col = 'Column number')\ndata_dict","metadata":{"execution":{"iopub.status.busy":"2023-02-27T12:39:02.667009Z","iopub.execute_input":"2023-02-27T12:39:02.667564Z","iopub.status.idle":"2023-02-27T12:39:02.702300Z","shell.execute_reply.started":"2023-02-27T12:39:02.667501Z","shell.execute_reply":"2023-02-27T12:39:02.700675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Split Data For Hyperparameter Tuning","metadata":{}},{"cell_type":"markdown","source":"Before we start data analysis, we should always separate our data into a training, validation and testing data set. For the hyperparameter tuning of our model, we first divide data into train and validation data set. ","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Split dataset to train and validation dataframe\ndf_train, df_val = train_test_split(df,test_size=0.3,random_state=0)\n\n# For safe editing\ndf_train_org = df_train.copy()","metadata":{"execution":{"iopub.status.busy":"2023-02-27T12:39:02.704174Z","iopub.execute_input":"2023-02-27T12:39:02.704616Z","iopub.status.idle":"2023-02-27T12:39:03.537956Z","shell.execute_reply.started":"2023-02-27T12:39:02.704577Z","shell.execute_reply":"2023-02-27T12:39:03.536426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_org","metadata":{"execution":{"iopub.status.busy":"2023-02-27T12:39:03.540033Z","iopub.execute_input":"2023-02-27T12:39:03.540626Z","iopub.status.idle":"2023-02-27T12:39:03.578065Z","shell.execute_reply.started":"2023-02-27T12:39:03.540569Z","shell.execute_reply":"2023-02-27T12:39:03.575849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's edit train. To start, we create the target variable by creating a column named 'problem_loan'. It should contain a 1 if the loan has an interest rate of more than 14%, i.e. if 'int_rate' > 14. Otherwise it should contain 0.","metadata":{}},{"cell_type":"code","source":"# Create the target variable\nmask=(df_train.loc[:,'int_rate'] > 14).astype('int8')\ndf_train.loc[:,'problem_loan']=mask\nprint('Number of Problem Loans in train data set: ',df_train.loc[:,'problem_loan'].sum())","metadata":{"execution":{"iopub.status.busy":"2023-02-27T12:39:03.580454Z","iopub.execute_input":"2023-02-27T12:39:03.581027Z","iopub.status.idle":"2023-02-27T12:39:03.596468Z","shell.execute_reply.started":"2023-02-27T12:39:03.580972Z","shell.execute_reply":"2023-02-27T12:39:03.594846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get an idea of target category: IsBadBuy\nplt.figure(figsize=(15,6))\nproblem_loan_share = df_train[\"problem_loan\"].value_counts()\nmylabel=[\"Not Problem Loan(0)\",\"Problem Loan(1)\"]\ncolors = ['#99ff99','#ff9999']\nplt.pie(problem_loan_share,\n        labels=mylabel,autopct=\"%1.1f%%\",colors=colors,\n        textprops={'fontsize': 16})\nplt.axis(\"equal\");","metadata":{"execution":{"iopub.status.busy":"2023-02-27T12:39:03.597870Z","iopub.execute_input":"2023-02-27T12:39:03.598896Z","iopub.status.idle":"2023-02-27T12:39:03.867620Z","shell.execute_reply.started":"2023-02-27T12:39:03.598823Z","shell.execute_reply":"2023-02-27T12:39:03.866406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the percentage of the missing values\npercent_missing = df_train.isnull().sum() * 100 / len(df_train)\nmissing_value_df = pd.DataFrame({'percent_missing (%)': percent_missing})\nmissing_value_df.sort_values('percent_missing (%)', ascending=False)","metadata":{"execution":{"iopub.status.busy":"2023-02-27T12:39:03.869004Z","iopub.execute_input":"2023-02-27T12:39:03.869377Z","iopub.status.idle":"2023-02-27T12:39:04.108370Z","shell.execute_reply.started":"2023-02-27T12:39:03.869323Z","shell.execute_reply":"2023-02-27T12:39:04.106307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the datatype\ndf.info()","metadata":{"execution":{"iopub.status.busy":"2023-02-27T12:39:04.114143Z","iopub.execute_input":"2023-02-27T12:39:04.115195Z","iopub.status.idle":"2023-02-27T12:39:04.400612Z","shell.execute_reply.started":"2023-02-27T12:39:04.115139Z","shell.execute_reply":"2023-02-27T12:39:04.399176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The type object appears quite frequently. Lets take a look!\n","metadata":{}},{"cell_type":"markdown","source":"# Data Cleaning Function\n\nIn order to simplify the required data cleaning steps, we are going to define a new function called data_cleaner which is responsible for this work. However, we need to investigate the features to better define the cleaning function.","metadata":{}},{"cell_type":"code","source":"# Check out for title and purpose columns\ndf_train.loc[:20, ['title', 'purpose']]","metadata":{"execution":{"iopub.status.busy":"2023-02-27T12:39:04.402249Z","iopub.execute_input":"2023-02-27T12:39:04.403097Z","iopub.status.idle":"2023-02-27T12:39:04.435250Z","shell.execute_reply.started":"2023-02-27T12:39:04.403047Z","shell.execute_reply":"2023-02-27T12:39:04.433868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check number of unique values\ndf_train.loc[: , ['title', 'purpose']].nunique()","metadata":{"execution":{"iopub.status.busy":"2023-02-27T12:39:04.437555Z","iopub.execute_input":"2023-02-27T12:39:04.438678Z","iopub.status.idle":"2023-02-27T12:39:04.570824Z","shell.execute_reply.started":"2023-02-27T12:39:04.438630Z","shell.execute_reply":"2023-02-27T12:39:04.569296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"'title' has 45592 unique values, while 'purpose' only has 14. This is because 'title' is generated by a free text field. For 'purpose' the choice is limited. Most loan seekers enter something similar for 'title'. So we only want to limit ourselves to 'purpose' since it is the concrete version of 'title'.\n\nBack to purpose. Lets see what percentage of the total data do each of the 14 reasons have.","metadata":{}},{"cell_type":"code","source":"# Dataframe with ratios \nfig, ax = plt.subplots(1, figsize=(6,5))  # define figure and  axes\ntrain_purpose_ratios = pd.crosstab(index=df_train.loc[:,'purpose'],\n                                   columns='ratio', normalize='columns').sort_values(by='ratio')\n\n# Plot ratios as horizontal bar graph\ntrain_purpose_ratios.plot(kind='barh', legend=False, ax=ax)  \n\n# improve plot\nax.set(xlabel='ratio', title='purposes ranked by ratio')\n\n# draw 1% line\nax.vlines(x=0.01, ymin=-0.5, ymax=13.5, color ='orange');  ","metadata":{"execution":{"iopub.status.busy":"2023-02-27T12:39:04.573136Z","iopub.execute_input":"2023-02-27T12:39:04.573587Z","iopub.status.idle":"2023-02-27T12:39:05.133378Z","shell.execute_reply.started":"2023-02-27T12:39:04.573547Z","shell.execute_reply":"2023-02-27T12:39:05.131920Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"'dept_consolidation' is by far the most frequently given reason. [Debt consolidation](https://en.wikipedia.org/wiki/Debt_consolidation) is a form of debt refinancing that entails taking out one loan to pay off many others.\n\nTo make it easier for us, we summarize the categories that make up less than 1% (orange line) as 'other'. To do this, run the following code cell.","metadata":{}},{"cell_type":"code","source":"# create mask and change values\nmask_purpose = df_train.loc[: , 'purpose'].isin(['educational', 'renewable_energy', 'house', 'wedding', 'vacation',\n       'moving', 'medical'])  \ndf_train.loc[mask_purpose, 'purpose'] = 'other'\n\n# check unique values\ndf_train['purpose'].unique()  ","metadata":{"execution":{"iopub.status.busy":"2023-02-27T12:39:05.135186Z","iopub.execute_input":"2023-02-27T12:39:05.135700Z","iopub.status.idle":"2023-02-27T12:39:05.189874Z","shell.execute_reply.started":"2023-02-27T12:39:05.135649Z","shell.execute_reply":"2023-02-27T12:39:05.188429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We need to simplify the zip code by using the first digit of it.","metadata":{}},{"cell_type":"code","source":"# Define new column 1d_zip to get the first digit of zip code\ndf_train.loc[:,'1d_zip'] = df_train.loc[:,'zip_code'].str[0]\ndf_train.loc[:,'1d_zip'].nunique()","metadata":{"execution":{"iopub.status.busy":"2023-02-27T12:39:05.191785Z","iopub.execute_input":"2023-02-27T12:39:05.192322Z","iopub.status.idle":"2023-02-27T12:39:05.458915Z","shell.execute_reply.started":"2023-02-27T12:39:05.192268Z","shell.execute_reply":"2023-02-27T12:39:05.457327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we come to the 'issue_d' column. Both the month and the year of the start of the loan are included here. We split these two pieces of information. The month is the first 3 characters of each value and the year is the last 4 characters. ","metadata":{}},{"cell_type":"code","source":"# Define column mounth and year\ndf_train.loc[:, 'month'] = df_train.loc[:, 'issue_d'].str[:3]\ndf_train.loc[:, 'year'] = pd.to_numeric(df_train.loc[:, 'issue_d'].str[-4:])","metadata":{"execution":{"iopub.status.busy":"2023-02-27T12:39:05.461468Z","iopub.execute_input":"2023-02-27T12:39:05.462056Z","iopub.status.idle":"2023-02-27T12:39:05.895729Z","shell.execute_reply.started":"2023-02-27T12:39:05.461995Z","shell.execute_reply":"2023-02-27T12:39:05.893915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create dataframe with ratios and plot\nfig, ax = plt.subplots(1, figsize=(6,5)) \npd.crosstab(index= df_train['year'], columns=df_train['problem_loan']).plot(kind='bar', ax=ax)\n\n# improve plot\nax.xaxis.set_tick_params(rotation=45)  # rotate labels of xaxis\nax.set(ylabel='count', title='Ratio of high interest loans rises');","metadata":{"execution":{"iopub.status.busy":"2023-02-27T12:39:05.897545Z","iopub.execute_input":"2023-02-27T12:39:05.897987Z","iopub.status.idle":"2023-02-27T12:39:06.191112Z","shell.execute_reply.started":"2023-02-27T12:39:05.897947Z","shell.execute_reply":"2023-02-27T12:39:06.189653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, number of loans have grown significantly in recent years (until 2014).","metadata":{}},{"cell_type":"code","source":"# Check emp_length, duration of the borrower's \n# employment at the start of the loan in years\ndf_train.loc[:, 'emp_length'].unique()","metadata":{"execution":{"iopub.status.busy":"2023-02-27T12:39:06.192988Z","iopub.execute_input":"2023-02-27T12:39:06.193552Z","iopub.status.idle":"2023-02-27T12:39:06.226704Z","shell.execute_reply.started":"2023-02-27T12:39:06.193500Z","shell.execute_reply":"2023-02-27T12:39:06.224952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract numeric values of emp_length not emp_length_num\n# Replace the '< 1' indications in 'emp_length' with '0'.\ndf_train.loc[:, 'emp_length'] = df_train.loc[:,'emp_length'].str.replace(pat=r'< 1', repl='0')\ndf_train.loc[:,'emp_length_num'] = df_train.loc[:,'emp_length'].str.extract(r'(\\d+)', expand=False)\n\n# Create new column unemployed\n# Fill with 0 (person is employed)\ndf_train.loc[:, 'unemployed'] = 0 \n# Select rows with missing job title and employment length\nmask_unemployed = (df_train.loc[:, 'emp_length'].isna()) & (df_train.loc[:, 'emp_title'].isna())  \n# Fill selected rows with 0 (person is not employed)\ndf_train.loc[mask_unemployed, 'unemployed'] = 1  \n\n# Delete rows with employment length but missing job title\nmask_na = (df_train.loc[:, 'emp_title'].isna()) & (~df_train.loc[:, 'emp_length'].isna())\ndf_train = df_train.drop(df_train.index[mask_na])\n\n# Delete rows with job title but missing employment length\nmask_na = (~df_train.loc[:, 'emp_title'].isna()) & (df_train.loc[:, 'emp_length'].isna())\ndf_train = df_train.drop(df_train.index[mask_na])\n\n# Fill missing values in emp_length_num with -1 to prevent dropping them\ndf_train.loc[:, 'emp_length_num'] = df_train.loc[:, 'emp_length_num'].fillna(-1)\ndf_train.loc[:, 'emp_length_num'] = pd.to_numeric(df_train.loc[:, 'emp_length_num'])","metadata":{"execution":{"iopub.status.busy":"2023-02-27T12:39:06.229560Z","iopub.execute_input":"2023-02-27T12:39:06.229998Z","iopub.status.idle":"2023-02-27T12:39:07.589928Z","shell.execute_reply.started":"2023-02-27T12:39:06.229959Z","shell.execute_reply":"2023-02-27T12:39:07.588483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create dataframe with ratios and plot\nfig, ax = plt.subplots(1, figsize=(6,5)) \npd.crosstab(index= df_train['emp_length_num'], columns=df_train['problem_loan'], normalize='index').plot(kind='bar', ax=ax)\n\n# Improve plot\n# Rotate labels of xaxis\nax.xaxis.set_tick_params(rotation=0) \nax.set(ylabel='ratio', title='Small differences in ratio of high interest loans');","metadata":{"execution":{"iopub.status.busy":"2023-02-27T12:39:07.591846Z","iopub.execute_input":"2023-02-27T12:39:07.592271Z","iopub.status.idle":"2023-02-27T12:39:07.934889Z","shell.execute_reply.started":"2023-02-27T12:39:07.592233Z","shell.execute_reply":"2023-02-27T12:39:07.933222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It can be seen that the proportions of normal loans and problem loans are distributed relatively equally across all employment duration groups. The proportion of problem loans is slightly higher for people without a job title and without a period of employment (-1). Although normal loans tend to predominate, there are comparatively many problem loans,","metadata":{}},{"cell_type":"markdown","source":"Now let's look at how many unique values ​​the remaining object columns have. To do this, run the following cell:","metadata":{}},{"cell_type":"code","source":"# Number of unique vlaues\ndf_train.loc[:, ['term', 'emp_title', 'grade', 'sub_grade', 'home_ownership', 'verification_status']].nunique()","metadata":{"execution":{"iopub.status.busy":"2023-02-27T12:39:07.936372Z","iopub.execute_input":"2023-02-27T12:39:07.936791Z","iopub.status.idle":"2023-02-27T12:39:08.236984Z","shell.execute_reply.started":"2023-02-27T12:39:07.936753Z","shell.execute_reply":"2023-02-27T12:39:08.235233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that 'emp_title' has a large number of unique values. Let's look at the first 20 values of this column.","metadata":{}},{"cell_type":"code","source":"# Print first 20 entries of 'emp_title'\ndf_train.loc[:,'emp_title'].head(20)","metadata":{"execution":{"iopub.status.busy":"2023-02-27T12:39:08.238717Z","iopub.execute_input":"2023-02-27T12:39:08.239409Z","iopub.status.idle":"2023-02-27T12:39:08.249562Z","shell.execute_reply.started":"2023-02-27T12:39:08.239366Z","shell.execute_reply":"2023-02-27T12:39:08.248269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that, similar to the 'title' column, many entries have the same meaning and are just spelled differently. Presumably this column was also created by a free text field. A first indication of the irrelevance of 'emp_title'. Furthermore, it stands to reason that a job title doesn't carry much information (especially these days). Salary and length of employment are more important to the bank. So we decide to consider 'emp_title' irrelevant and remove it. We will do this later. Let's look at the other columns first.","metadata":{}},{"cell_type":"code","source":"# Check the column term: number of repayment installments in months (36 or 60)\nprint(df_train.loc[:, 'term'].unique())\n\n# Replace the values ​​in 'term' with the number of months, i.e. 36 or 60\ndf_train.loc[:, 'term'] = df_train.loc[:, 'term'].replace({' 36 months': 36, ' 60 months': 60})\ndf_train.loc[:, 'term'].dtype","metadata":{"execution":{"iopub.status.busy":"2023-02-27T12:39:08.251149Z","iopub.execute_input":"2023-02-27T12:39:08.251841Z","iopub.status.idle":"2023-02-27T12:39:08.472202Z","shell.execute_reply.started":"2023-02-27T12:39:08.251796Z","shell.execute_reply":"2023-02-27T12:39:08.470578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The columns 'year' and 'month' should not be used in the model for the time being. Your superiors are convinced that seasonal effects play a minor role here. The growth in high-interest loans in recent years is mainly due to the marketing strategy. \n\n### Columns to drop\nSo we no longer need the columns ['id', 'issue_d', 'month', 'year', 'emp_title', 'emp_length', 'title', 'zip_code', 'int_rate'] any more. ","metadata":{}},{"cell_type":"code","source":"# Drop undesired columns\ndf_train = df_train.drop(['id', 'issue_d', 'month', 'year', 'emp_title', 'emp_length', 'title', 'zip_code', 'int_rate'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-02-27T12:39:08.474594Z","iopub.execute_input":"2023-02-27T12:39:08.475106Z","iopub.status.idle":"2023-02-27T12:39:08.531312Z","shell.execute_reply.started":"2023-02-27T12:39:08.475052Z","shell.execute_reply":"2023-02-27T12:39:08.529173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the number of nan\ndf_train.isna().sum().sort_values(ascending=False)","metadata":{"execution":{"iopub.status.busy":"2023-02-27T12:39:08.533595Z","iopub.execute_input":"2023-02-27T12:39:08.534593Z","iopub.status.idle":"2023-02-27T12:39:08.648182Z","shell.execute_reply.started":"2023-02-27T12:39:08.534544Z","shell.execute_reply":"2023-02-27T12:39:08.646413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The columns 'percent_bc_gt_75' and 'total_bc_limit' still have many missing values. We don't know exactly where they come from. Both columns are about credit cards. It could be that these are mainly people who do not have credit cards. However, the number of missing values in both columns differs by almost 4000. So let's make it simple and remove the missing values","metadata":{}},{"cell_type":"code","source":"# Drop percent_bc_gt_75 and total_bc_limit\ndf_train = df_train.dropna()","metadata":{"execution":{"iopub.status.busy":"2023-02-27T12:39:08.650613Z","iopub.execute_input":"2023-02-27T12:39:08.651171Z","iopub.status.idle":"2023-02-27T12:39:08.812498Z","shell.execute_reply.started":"2023-02-27T12:39:08.651119Z","shell.execute_reply":"2023-02-27T12:39:08.810488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define required data cleaning function\n\ndef data_cleaner(df):\n    '''\n    Function data_cleaner is responsible for train and validation data cleaning.\n    Input:\n        df: Train/validation dataframe\n    output:\n        df_cleaned: Cleaned train/validation dataframe   \n    '''\n    copy=df.copy()\n    \n    # 1-Create target column 'problem_loan'. loans with interest rate\n    # of 15 or higher are assigned as problem_loa with 1 otherwise with 0.\n    mask=(copy.loc[:,'int_rate'] > 14).astype('int8')\n    copy.loc[:,'problem_loan']=mask\n    \n    # 2-Summarize the categories that make up less than 1%  of 'purpose' as 'other'.\n    mask_purpose = copy.loc[: , 'purpose'].isin(['educational', 'renewable_energy', 'house', 'wedding', 'vacation',\n       'moving', 'medical'])  \n    copy.loc[mask_purpose, 'purpose'] = 'other'\n    \n    # 3-Get only first number of zip code as discussed in notebook\n    copy.loc[:,'1d_zip'] = copy.loc[:,'zip_code'].str[0]\n    \n    # 4-Replace term with numbers:  36 months': 36, ' 60 months': 60\n    copy.loc[:, 'term'] = copy.loc[:, 'term'].replace({' 36 months': 36, ' 60 months': 60})\n    \n    # 5-Extract numeric values of emp_length into emp_length_num\n    copy.loc[:, 'emp_length'] = copy.loc[:,'emp_length'].str.replace(pat=r'< 1', repl='0')\n    copy.loc[:,'emp_length_num'] = copy.loc[:,'emp_length'].str.extract(r'(\\d+)', expand=False)\n    \n    # 6-Create new column unemployed\n    # Fill with 0 (person is employed)\n    copy.loc[:, 'unemployed'] = 0 \n    # Select rows with missing job title and employment length\n    mask_unemployed = (copy.loc[:, 'emp_length'].isna()) & (copy.loc[:, 'emp_title'].isna())  \n    # Fill selected rows with 0 (person is not employed)\n    copy.loc[mask_unemployed, 'unemployed'] = 1  \n\n    # 7-Delete rows with employment length but missing job title\n    mask_na = (copy.loc[:, 'emp_title'].isna()) & (~copy.loc[:, 'emp_length'].isna())\n    copy = copy.drop(copy.index[mask_na])\n\n    # 8-Delete rows with job title but missing employment length\n    mask_na = (~copy.loc[:, 'emp_title'].isna()) & (copy.loc[:, 'emp_length'].isna())\n    copy = copy.drop(copy.index[mask_na])\n\n    # 9-Fill missing values in emp_length_num with -1 to prevent dropping them\n    copy.loc[:, 'emp_length_num'] = copy.loc[:, 'emp_length_num'].fillna(-1)\n    copy.loc[:, 'emp_length_num'] = pd.to_numeric(copy.loc[:, 'emp_length_num'])\n    \n    # 10-Drop irrelevant columns. We have not introduced `year` and `month`, thus we do not need to drop them\n    copy = copy.drop(['id', 'issue_d', 'emp_title', 'emp_length', 'title', 'zip_code', 'int_rate'], axis=1)\n    \n    # 11-Drop missing values    \n    df_cleaned = copy.dropna()\n    \n    return df_cleaned   ","metadata":{"execution":{"iopub.status.busy":"2023-02-27T12:39:08.814998Z","iopub.execute_input":"2023-02-27T12:39:08.815622Z","iopub.status.idle":"2023-02-27T12:39:08.837697Z","shell.execute_reply.started":"2023-02-27T12:39:08.815566Z","shell.execute_reply":"2023-02-27T12:39:08.835311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Apply data_cleaner on both train and validation data.","metadata":{}},{"cell_type":"code","source":"clean_loans_train = data_cleaner(df_train_org)\nclean_loans_val = data_cleaner(df_val)","metadata":{"execution":{"iopub.status.busy":"2023-02-27T12:39:08.846186Z","iopub.execute_input":"2023-02-27T12:39:08.847281Z","iopub.status.idle":"2023-02-27T12:39:11.591710Z","shell.execute_reply.started":"2023-02-27T12:39:08.847231Z","shell.execute_reply":"2023-02-27T12:39:11.590210Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Encode Categorical Features","metadata":{}},{"cell_type":"code","source":"# Import relevant transformers\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Define columns for OHE\ncolumns=['grade', 'sub_grade', 'home_ownership', 'verification_status','1d_zip', 'purpose']\n\n# Set up pipeline\nohe = OneHotEncoder(sparse=False)\nencoder = ColumnTransformer([('OHE',ohe, columns)], remainder='passthrough')\n\n# Fit encoder\nencoder.fit(clean_loans_train)\n\n# Restore column names for final DataFrames\nohe_names = encoder.named_transformers_['OHE'].get_feature_names(columns)\nremaining_names = clean_loans_train.columns[encoder._remainder[2]]\n\n# Apply encoding and create DataFrames\nfinal_loans_train = pd.DataFrame(encoder.transform(clean_loans_train), columns = list(ohe_names) + list(remaining_names))\nfinal_loans_val = pd.DataFrame(encoder.transform(clean_loans_val), columns = list(ohe_names) + list(remaining_names))\n\n# Check shapes\nprint(final_loans_train.shape)\nprint(final_loans_val.shape)","metadata":{"execution":{"iopub.status.busy":"2023-02-27T12:39:11.594244Z","iopub.execute_input":"2023-02-27T12:39:11.594709Z","iopub.status.idle":"2023-02-27T12:39:13.794068Z","shell.execute_reply.started":"2023-02-27T12:39:11.594671Z","shell.execute_reply":"2023-02-27T12:39:13.792595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split train and validation data into features and target\ntarget_train = final_loans_train.loc[:, 'problem_loan']\nfeatures_train = final_loans_train.drop('problem_loan', axis=1)\ntarget_val = final_loans_val.loc[:, 'problem_loan']\nfeatures_val = final_loans_val.drop('problem_loan', axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-02-27T12:39:13.795702Z","iopub.execute_input":"2023-02-27T12:39:13.796139Z","iopub.status.idle":"2023-02-27T12:39:13.950970Z","shell.execute_reply.started":"2023-02-27T12:39:13.796100Z","shell.execute_reply":"2023-02-27T12:39:13.949465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Artificial Neural Networks\n","metadata":{}},{"cell_type":"markdown","source":"## Comparing Artificial Neuron versus Logistic Regression\n\nLet's check the claim that logistic regression is the same model as an artificial neuron with the sigmoid activation function. We do this by comparing the results of both models directly with each other and at the same time creating a first baseline model.\n\nSince we have a lot of binary features, features_train and features_val should first be scaled with the MinMaxScaler from sklearn.preprocessing.\n","metadata":{}},{"cell_type":"code","source":"# Scale train and validation data\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nfeatures_train_scaled = scaler.fit_transform(features_train)\nfeatures_val_scaled = scaler.transform(features_val)","metadata":{"execution":{"iopub.status.busy":"2023-02-27T13:03:27.742981Z","iopub.execute_input":"2023-02-27T13:03:27.743547Z","iopub.status.idle":"2023-02-27T13:03:28.087292Z","shell.execute_reply.started":"2023-02-27T13:03:27.743503Z","shell.execute_reply":"2023-02-27T13:03:28.085872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initiate Logistic Regression \nfrom sklearn.linear_model import LogisticRegression\n\nmodel_log = LogisticRegression(solver='lbfgs', max_iter=1000, C=1e42)\nmodel_log.fit(features_train_scaled, target_train)\n\ntarget_val_pred_log = model_log.predict(features_val_scaled)\ntarget_val_pred_log","metadata":{"execution":{"iopub.status.busy":"2023-02-27T13:03:58.120105Z","iopub.execute_input":"2023-02-27T13:03:58.120786Z","iopub.status.idle":"2023-02-27T13:04:34.047295Z","shell.execute_reply.started":"2023-02-27T13:03:58.120730Z","shell.execute_reply":"2023-02-27T13:04:34.045796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since we have a almost balanced class distribution in our data, we can use accuracy as a good evaluation metric. Now import accuracy_score from sklearn.metrics and print the achieved accuracy on the validation set.","metadata":{}},{"cell_type":"code","source":"# Model accuracy score on validation data\nfrom sklearn.metrics import accuracy_score\naccuracy_score(target_val, target_val_pred_log)","metadata":{"execution":{"iopub.status.busy":"2023-02-27T13:05:15.947665Z","iopub.execute_input":"2023-02-27T13:05:15.948099Z","iopub.status.idle":"2023-02-27T13:05:15.979417Z","shell.execute_reply.started":"2023-02-27T13:05:15.948066Z","shell.execute_reply":"2023-02-27T13:05:15.978074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We use Tensorflow for artificial neuron. Tensorflow is currently the most extensive and widespread tool for the realization of models with artificial neurons. Probably the biggest advantage of tensorflow is that it includes the keras module as an API. Because of keras, creating models with artificial neurons is very easy, since the handling is very similar to sklearn.\n","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n\n# Define the model type\nmodel_an = Sequential()  \n# Add one artificial neuron with sigmoid activtaion function\nmodel_an.add(Dense(1, activation='sigmoid', input_dim=features_train_scaled.shape[1]))  \n # Compile the model\nmodel_an.compile(optimizer = \"adam\", loss = 'binary_crossentropy', metrics = ['accuracy']) \n","metadata":{"execution":{"iopub.status.busy":"2023-02-27T13:06:58.734583Z","iopub.execute_input":"2023-02-27T13:06:58.735668Z","iopub.status.idle":"2023-02-27T13:07:09.715424Z","shell.execute_reply.started":"2023-02-27T13:06:58.735618Z","shell.execute_reply":"2023-02-27T13:07:09.713807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To train it, we can use the model_an.fit() method, analogous to sklearn.","metadata":{}},{"cell_type":"code","source":"# Fit the model\nmodel_an.fit(features_train_scaled, target_train, epochs=5) ","metadata":{"execution":{"iopub.status.busy":"2023-02-27T13:12:42.407527Z","iopub.execute_input":"2023-02-27T13:12:42.408043Z","iopub.status.idle":"2023-02-27T13:14:02.877985Z","shell.execute_reply.started":"2023-02-27T13:12:42.407997Z","shell.execute_reply":"2023-02-27T13:14:02.876555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's compare our artificial neuron to logistic regression. For this we need the predictions of model_an on the validation data.","metadata":{}},{"cell_type":"code","source":"# Predict AN model\ntarget_val_pred_an = model_an.predict(features_val_scaled)\ntarget_val_pred_an","metadata":{"execution":{"iopub.status.busy":"2023-02-27T13:14:21.050180Z","iopub.execute_input":"2023-02-27T13:14:21.051581Z","iopub.status.idle":"2023-02-27T13:14:27.527549Z","shell.execute_reply.started":"2023-02-27T13:14:21.051522Z","shell.execute_reply":"2023-02-27T13:14:27.526219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert results into a 1-dimensional array\ntarget_val_pred_an = target_val_pred_an.flatten()\ntarget_val_pred_an","metadata":{"execution":{"iopub.status.busy":"2023-02-27T13:16:09.368773Z","iopub.execute_input":"2023-02-27T13:16:09.370259Z","iopub.status.idle":"2023-02-27T13:16:09.379360Z","shell.execute_reply.started":"2023-02-27T13:16:09.370204Z","shell.execute_reply":"2023-02-27T13:16:09.377780Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To calculate the accuracy, we now need to convert the predictions into the discrete values 0 and 1. To do this, we use the limit value 0.5, as in LogisticRegression. So replace all values less than 0.5 in target_val_pred_an with 0 and the remaining values with 1. Print out the result.","metadata":{}},{"cell_type":"code","source":"target_val_pred_an = target_val_pred_an > 0.5\naccuracy_score(target_val, target_val_pred_an)","metadata":{"execution":{"iopub.status.busy":"2023-02-27T13:18:03.226341Z","iopub.execute_input":"2023-02-27T13:18:03.227490Z","iopub.status.idle":"2023-02-27T13:18:03.260888Z","shell.execute_reply.started":"2023-02-27T13:18:03.227437Z","shell.execute_reply":"2023-02-27T13:18:03.259398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"tensorflow also provides us with a shortcut of the above steps. Instead of first generating the predictions, then converting the values to discrete values and finally calculating the accuracy, we can call the my_model.evaluate() method. This takes the validation data as input and outputs two values.","metadata":{}},{"cell_type":"code","source":"# Evaluate the model\nmodel_an.evaluate(features_val_scaled, target_val)","metadata":{"execution":{"iopub.status.busy":"2023-02-27T13:31:21.467889Z","iopub.execute_input":"2023-02-27T13:31:21.469410Z","iopub.status.idle":"2023-02-27T13:31:28.085997Z","shell.execute_reply.started":"2023-02-27T13:31:21.469337Z","shell.execute_reply":"2023-02-27T13:31:28.084620Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The artificial neuron also achieves an almost identical accuracy score of approx. 92.7% (the second value) with the sigmoid activation function (slight deviations are always possible due to statistical fluctuations). So our assumption that an artificial neuron with sigmoid activation function is identical to logistic regression is correct - both theoretically and mathematically.","metadata":{}},{"cell_type":"markdown","source":"# Apply Artificial Neural Networks - Feedforward Neural Network (FNN)\n\nA[ Feed Forward Neural Network](https://deepai.org/machine-learning-glossary-and-terms/feed-forward-neural-network) is an artificial neural network in which the connections between nodes does not form a cycle. The opposite of a feed forward neural network is a recurrent neural network, in which certain pathways are cycled. The feed forward model is the simplest form of neural network as information is only processed in one direction. While the data may pass through multiple hidden nodes, it always moves in one direction and never backwards.\n![](https://images.deepai.org/django-summernote/2019-06-06/5c17d9c2-0ad4-474c-be8d-d6ae9b094e74.png)\n","metadata":{}},{"cell_type":"markdown","source":"# ... To be continued","metadata":{}}]}