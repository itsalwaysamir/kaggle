{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72126dfa",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-02-22T16:50:32.737765Z",
     "iopub.status.busy": "2023-02-22T16:50:32.737313Z",
     "iopub.status.idle": "2023-02-22T16:50:32.753066Z",
     "shell.execute_reply": "2023-02-22T16:50:32.751660Z"
    },
    "papermill": {
     "duration": 0.02919,
     "end_time": "2023-02-22T16:50:32.755740",
     "exception": false,
     "start_time": "2023-02-22T16:50:32.726550",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input/spam-email/spam.csv'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e472a465",
   "metadata": {
    "papermill": {
     "duration": 0.004633,
     "end_time": "2023-02-22T16:50:32.765643",
     "exception": false,
     "start_time": "2023-02-22T16:50:32.761010",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Scenario\n",
    "Company dislikeSpam approaches us to improve their customer service by allowing their customers to reach out to them with text messages. However, since they are afraid that they will receive a lot of spam messages, they would like to recognize and filter them automatically. Our task is to build a first prototype and create a proof of concept.\n",
    "\n",
    "![](http://www.dww.com/sites/default/files/styles/landscape_ri/public/shutterstock_172545959.jpg?itok=mPrJKywC)\n",
    "Image source: http://www.dww.com/sites/default/files/styles/landscape_ri/public/shutterstock_172545959.jpg?itok=mPrJKywC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd83d6d",
   "metadata": {
    "papermill": {
     "duration": 0.004452,
     "end_time": "2023-02-22T16:50:32.774925",
     "exception": false,
     "start_time": "2023-02-22T16:50:32.770473",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Understand Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "966ed2ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-22T16:50:32.786334Z",
     "iopub.status.busy": "2023-02-22T16:50:32.785927Z",
     "iopub.status.idle": "2023-02-22T16:50:33.823309Z",
     "shell.execute_reply": "2023-02-22T16:50:33.822179Z"
    },
    "papermill": {
     "duration": 1.048031,
     "end_time": "2023-02-22T16:50:33.827918",
     "exception": false,
     "start_time": "2023-02-22T16:50:32.779887",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Category                                            Message\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import regured modules \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# suppress scientific notation\n",
    "np.set_printoptions(suppress=True) \n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "\n",
    "# Read Data\n",
    "df = pd.read_csv('/kaggle/input/spam-email/spam.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750f40df",
   "metadata": {
    "papermill": {
     "duration": 0.004977,
     "end_time": "2023-02-22T16:50:33.838638",
     "exception": false,
     "start_time": "2023-02-22T16:50:33.833661",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Note:\n",
    "What is ham and spam?\n",
    "\"Ham\" is e-mail that is not Spam. In other words, \"non-spam\", or \"good mail\". It should be considered a shorter, snappier synonym for \"non-spam\". Its usage is particularly common among anti-spam software developers, and not widely known elsewhere; in general it is probably better to use the term \"non-spam\", instead.\n",
    "\n",
    "[source](https://cwiki.apache.org/confluence/display/spamassassin/Ham#:~:text=%22Ham%22%20is%20e%2Dmail,non%2Dspam%22%2C%20instead.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7af9402",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-22T16:50:33.850859Z",
     "iopub.status.busy": "2023-02-22T16:50:33.850485Z",
     "iopub.status.idle": "2023-02-22T16:50:33.856824Z",
     "shell.execute_reply": "2023-02-22T16:50:33.856071Z"
    },
    "papermill": {
     "duration": 0.01492,
     "end_time": "2023-02-22T16:50:33.858863",
     "exception": false,
     "start_time": "2023-02-22T16:50:33.843943",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DataFrame shape - Number of rows and columns\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5d982ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-22T16:50:33.871816Z",
     "iopub.status.busy": "2023-02-22T16:50:33.871020Z",
     "iopub.status.idle": "2023-02-22T16:50:33.884139Z",
     "shell.execute_reply": "2023-02-22T16:50:33.882897Z"
    },
    "papermill": {
     "duration": 0.022289,
     "end_time": "2023-02-22T16:50:33.886660",
     "exception": false,
     "start_time": "2023-02-22T16:50:33.864371",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of not-spam message:  Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
      "Example of spam message:  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n"
     ]
    }
   ],
   "source": [
    "# Take a look at two example of not-spam and spam message\n",
    "print('Example of not-spam message: ', df.loc[0, 'Message'])\n",
    "print('Example of spam message: ', df.loc[2, 'Message'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14249ef6",
   "metadata": {
    "papermill": {
     "duration": 0.005724,
     "end_time": "2023-02-22T16:50:33.897902",
     "exception": false,
     "start_time": "2023-02-22T16:50:33.892178",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Cleaning and Prepration\n",
    "In this step, first, the text data is cleaned to correct errors and give it more structure for analysis (data cleaning and preparation). Then[ NLP methods](https://www.ibm.com/topics/natural-language-processing#:~:text=the%20next%20step-,What%20is%20natural%20language%20processing%3F,same%20way%20human%20beings%20can.) are used to get certain parts of the text that reflect the categories as best as possible. These extracted features can then be used with classification models. In this project, [Support Vector Machine](https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47) is going to be used as classifer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f351c270",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-22T16:50:33.910761Z",
     "iopub.status.busy": "2023-02-22T16:50:33.910367Z",
     "iopub.status.idle": "2023-02-22T16:50:47.496628Z",
     "shell.execute_reply": "2023-02-22T16:50:47.495320Z"
    },
    "papermill": {
     "duration": 13.596527,
     "end_time": "2023-02-22T16:50:47.499956",
     "exception": false,
     "start_time": "2023-02-22T16:50:33.903429",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import required modules for playing with strings\n",
    "import string\n",
    "import re\n",
    "\n",
    "# NLP modules\n",
    "import spacy\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6f7ff58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-22T16:50:47.522384Z",
     "iopub.status.busy": "2023-02-22T16:50:47.521254Z",
     "iopub.status.idle": "2023-02-22T16:50:48.373057Z",
     "shell.execute_reply": "2023-02-22T16:50:48.371715Z"
    },
    "papermill": {
     "duration": 0.866118,
     "end_time": "2023-02-22T16:50:48.375544",
     "exception": false,
     "start_time": "2023-02-22T16:50:47.509426",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.lang.en.English'>\n"
     ]
    }
   ],
   "source": [
    "#  Load spacy's English statistical model to enable NLP tasks for an English corpus\n",
    "from spacy.lang.en.examples import sentences\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "print(type(nlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209979f3",
   "metadata": {
    "papermill": {
     "duration": 0.005324,
     "end_time": "2023-02-22T16:50:48.386369",
     "exception": false,
     "start_time": "2023-02-22T16:50:48.381045",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We see that nlp has the data type Language. This means that it contains all the components necessary to process English-language text. Fpr more details please see [here](https://spacy.io/usage/models)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105cbe35",
   "metadata": {
    "papermill": {
     "duration": 0.005117,
     "end_time": "2023-02-22T16:50:48.396775",
     "exception": false,
     "start_time": "2023-02-22T16:50:48.391658",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Cleaning and processing techniques are:\n",
    "[Useful links:](https://www.turing.com/kb/natural-language-processing-function-in-ai)\n",
    "#### tokenization\n",
    "In order to properly analyze text data, machine learning models should be able to recognize structures in the text, such as individual words and their parts of speech. We achieve this through tokenization. The corpus is broken down into meaningful linguistic units, such as words or sentences, and saved as a list. The elements of this list are called tokens. \n",
    "\n",
    "#### Lemmatization\n",
    "Lemmatization means that words are reduced to their basic form, also known as lemma. For grammatical reasons, different forms of the same word can be used in one text, e.g. make, makes, making or maker. Python would consider these variations of the word make as separate words, even though their meaning is the same. Through lemmatization, we bring all variations of make to this basic form.\n",
    "#### Stop word removal\n",
    "Stop word removal is used to remove common words from a text. Stop words are usually articles (\"the\" and \"a\"), pronouns like \"I\" and \"you\" (already removed in the previous step), or common verbs (\"be\", \"can\"). These words are common in most English language texts. Removing these words would reduce the amount of data that needs to be analyzed while allowing machine learning algorithms to place more weight on tokens that give a text its real meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e63fa090",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-22T16:50:48.409530Z",
     "iopub.status.busy": "2023-02-22T16:50:48.409062Z",
     "iopub.status.idle": "2023-02-22T16:50:48.424313Z",
     "shell.execute_reply": "2023-02-22T16:50:48.422946Z"
    },
    "papermill": {
     "duration": 0.024697,
     "end_time": "2023-02-22T16:50:48.426671",
     "exception": false,
     "start_time": "2023-02-22T16:50:48.401974",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'now', \"didn't\", 've', 'these', 'mustn', 'each', 'such', 'further', 'our', 'y', \"hadn't\", 'in', 'some', 'you', \"you've\", 're', 'the', 'before', 'does', \"aren't\", 'wasn', 'him', 'themselves', 'no', \"don't\", 'didn', 'through', \"shouldn't\", 'she', 'itself', 'during', 'weren', 'did', \"wouldn't\", 'when', 'am', 'at', 'their', 'haven', 'all', 'hers', 'own', 'as', \"hasn't\", 'm', 'for', 'has', 'then', 'hadn', \"you're\", \"you'd\", 'against', \"haven't\", 'both', 'above', 'he', \"mightn't\", 'after', 'where', 'below', \"mustn't\", 'was', 'any', 'don', 'is', 'down', 'same', 'yourself', 'few', 'than', 'into', 'about', 'if', 'me', 'or', 'theirs', 'o', 'herself', 'yours', 'couldn', 'shouldn', \"needn't\", 'his', \"wasn't\", 'ourselves', 'have', 'ours', 'once', 'will', 'that', 'be', 'until', 'wouldn', 'having', \"it's\", 'doing', 'up', 'more', 'here', 'my', 'so', 'do', \"isn't\", 'had', 'under', 'ain', \"doesn't\", 'shan', 'those', 'there', 'not', 'a', \"should've\", 'them', 'an', 'your', \"weren't\", 'hasn', \"that'll\", 'why', 'by', 'should', \"she's\", 'what', 'on', \"couldn't\", 'while', 'its', 'aren', 'her', 'himself', 'of', 'with', 'they', 'mightn', 'won', \"shan't\", 'out', \"won't\", 'we', 'doesn', 'too', 'being', 'can', 'i', 'but', 'who', 'which', 'again', 'only', 'ma', 'from', 'whom', 'to', 'over', 'needn', 'are', 'just', 'because', 'other', 'very', \"you'll\", 'how', 'myself', 'most', 'were', 'yourselves', 'isn', 'between', 'nor', 'd', 'it', 'this', 'been', 'off', 't', 'and', 'll', 's'}\n"
     ]
    }
   ],
   "source": [
    "# Import stopwords from nltk\n",
    "from nltk.corpus import stopwords\n",
    "# Save stopwords as a set\n",
    "stopWords = set(stopwords.words('english'))\n",
    "type(stopWords)\n",
    "print(stopWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3423e4",
   "metadata": {
    "papermill": {
     "duration": 0.005109,
     "end_time": "2023-02-22T16:50:48.437317",
     "exception": false,
     "start_time": "2023-02-22T16:50:48.432208",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Removal of punctuation marks\n",
    "Just like stop word removal, punctuation removal involves removing punctuation marks and symbols that do not contribute to the meaning of the text. We can use punctuation from the string module. This is a string consisting of punctuation marks and symbols. We can remove these from our text like the stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbabc649",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-22T16:50:48.449812Z",
     "iopub.status.busy": "2023-02-22T16:50:48.449437Z",
     "iopub.status.idle": "2023-02-22T16:50:48.455243Z",
     "shell.execute_reply": "2023-02-22T16:50:48.453874Z"
    },
    "papermill": {
     "duration": 0.015072,
     "end_time": "2023-02-22T16:50:48.457650",
     "exception": false,
     "start_time": "2023-02-22T16:50:48.442578",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "# List of punctuation marks\n",
    "punctuations = string.punctuation\n",
    "print(punctuations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51c42b0",
   "metadata": {
    "papermill": {
     "duration": 0.005271,
     "end_time": "2023-02-22T16:50:48.468285",
     "exception": false,
     "start_time": "2023-02-22T16:50:48.463014",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Steps explained above are all packed into function message_cleaner is going to be used for cleaninf and processing of our text data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f10f9a5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-22T16:50:48.481238Z",
     "iopub.status.busy": "2023-02-22T16:50:48.480034Z",
     "iopub.status.idle": "2023-02-22T16:50:48.488230Z",
     "shell.execute_reply": "2023-02-22T16:50:48.487193Z"
    },
    "papermill": {
     "duration": 0.017236,
     "end_time": "2023-02-22T16:50:48.490726",
     "exception": false,
     "start_time": "2023-02-22T16:50:48.473490",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# `Function message_cleaner\n",
    "def message_cleaner(sentence):\n",
    "    \"\"\"\n",
    "    Function message_cleaner clean the text using typical Natural Language Processing\n",
    "    (NLP) steps. \n",
    "    Steps include: Lemmatization, removing stop words, removing punctuations  \n",
    "    Args:\n",
    "        sentence (str): The uncleaned text. \n",
    "    Returns:\n",
    "        str: The cleaned text.\n",
    "    \"\"\"\n",
    "    # Create the Doc object named `text` from `sentence` using `nlp()`\n",
    "    text = nlp(sentence)    \n",
    "    # Lemmatization - remove the lemmas -PRON-     \n",
    "    text = [ token.lemma_ for token in text if token.lemma_ != \"-PRON-\"]\n",
    "    # Remove stop words\n",
    "    text = [ token for token in text if token not in stopWords ]\n",
    "    # Remove punctuations\n",
    "    text = [ token for token in text if token not in punctuations]\n",
    "    # Use the .join() method on text to convert string\n",
    "    text = \" \".join(text)\n",
    "    # Use re.sub() to substitute multiple spaces or dots`[\\.\\s]+` to single space `' '`\n",
    "    text  = re.sub('[\\.\\s]+', ' ', text)\n",
    "    \n",
    "    # Return the cleaned text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af987af",
   "metadata": {
    "papermill": {
     "duration": 0.005579,
     "end_time": "2023-02-22T16:50:48.501963",
     "exception": false,
     "start_time": "2023-02-22T16:50:48.496384",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now it's time to apply our function to all messages in df. Apply my_series.apply() to the 'msg' column of df and pass it the text_cleaner function to apply it to all text messages. Store the results in a new column in df called 'msg_clean'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f93f0145",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-22T16:50:48.514632Z",
     "iopub.status.busy": "2023-02-22T16:50:48.514261Z",
     "iopub.status.idle": "2023-02-22T16:51:35.064429Z",
     "shell.execute_reply": "2023-02-22T16:51:35.063258Z"
    },
    "papermill": {
     "duration": 46.55988,
     "end_time": "2023-02-22T16:51:35.067250",
     "exception": false,
     "start_time": "2023-02-22T16:50:48.507370",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply function message_cleaner to all text messages and store the results in new column as message_cleaned\n",
    "df.loc[:,'message_cleaned'] = df.loc[:,'Message'].apply(message_cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2597b9a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-22T16:51:35.081560Z",
     "iopub.status.busy": "2023-02-22T16:51:35.081175Z",
     "iopub.status.idle": "2023-02-22T16:51:35.095665Z",
     "shell.execute_reply": "2023-02-22T16:51:35.094298Z"
    },
    "papermill": {
     "duration": 0.024649,
     "end_time": "2023-02-22T16:51:35.098177",
     "exception": false,
     "start_time": "2023-02-22T16:51:35.073528",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "      <th>message_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>go jurong point crazy available bugis n great ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ok lar joke wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry 2 wkly comp win FA Cup final tkts 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>u dun say early hor u c already say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah I think go usf live around though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "      <td>2nd time try 2 contact u u win £ 750 Pound pri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "      <td>ü b go esplanade fr home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "      <td>pity mood suggestion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "      <td>guy bitching I act like I would interested buy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "      <td>Rofl true name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Category                                            Message  \\\n",
       "0         ham  Go until jurong point, crazy.. Available only ...   \n",
       "1         ham                      Ok lar... Joking wif u oni...   \n",
       "2        spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3         ham  U dun say so early hor... U c already then say...   \n",
       "4         ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "...       ...                                                ...   \n",
       "5567     spam  This is the 2nd time we have tried 2 contact u...   \n",
       "5568      ham               Will ü b going to esplanade fr home?   \n",
       "5569      ham  Pity, * was in mood for that. So...any other s...   \n",
       "5570      ham  The guy did some bitching but I acted like i'd...   \n",
       "5571      ham                         Rofl. Its true to its name   \n",
       "\n",
       "                                        message_cleaned  \n",
       "0     go jurong point crazy available bugis n great ...  \n",
       "1                                ok lar joke wif u oni   \n",
       "2     free entry 2 wkly comp win FA Cup final tkts 2...  \n",
       "3                  u dun say early hor u c already say   \n",
       "4                 nah I think go usf live around though  \n",
       "...                                                 ...  \n",
       "5567  2nd time try 2 contact u u win £ 750 Pound pri...  \n",
       "5568                           ü b go esplanade fr home  \n",
       "5569                               pity mood suggestion  \n",
       "5570  guy bitching I act like I would interested buy...  \n",
       "5571                                     Rofl true name  \n",
       "\n",
       "[5572 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the results\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5a24f87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-22T16:51:35.111794Z",
     "iopub.status.busy": "2023-02-22T16:51:35.111389Z",
     "iopub.status.idle": "2023-02-22T16:51:35.116500Z",
     "shell.execute_reply": "2023-02-22T16:51:35.115128Z"
    },
    "papermill": {
     "duration": 0.014629,
     "end_time": "2023-02-22T16:51:35.118732",
     "exception": false,
     "start_time": "2023-02-22T16:51:35.104103",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 74.180139,
   "end_time": "2023-02-22T16:51:38.497734",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-02-22T16:50:24.317595",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
